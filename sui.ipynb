{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1685747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\Desktop\\PDF-RAG\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# notebooks/01_markdown_eda.ipynb\n",
    "# Exploratory Data Analysis for Markdown Documents\n",
    "\n",
    "# ==============================\n",
    "# 1. Setup & Imports\n",
    "# ==============================\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 2. Configuration\n",
    "# ==============================\n",
    "MARKDOWN_DIR = Path(\"data/markdown_docs\")\n",
    "\n",
    "# Boilerplate phrases to remove (extend carefully)\n",
    "BOILERPLATE_PATTERNS = [\n",
    "    r\"Lecture note prepared by.*\",\n",
    "    r\"Prepared by.*\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee6702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# 3. Utility Functions\n",
    "# ==============================\n",
    "\n",
    "def light_clean_markdown(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Light cleaning only:\n",
    "    - Remove extra blank lines\n",
    "    - Remove known boilerplate phrases\n",
    "    - Preserve code blocks and markdown syntax\n",
    "    \"\"\"\n",
    "    # Remove boilerplate lines\n",
    "    for pattern in BOILERPLATE_PATTERNS:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Normalize excessive blank lines (3+ -> 2)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def count_words(text: str) -> int:\n",
    "    return len(re.findall(r\"\\b\\w+\\b\", text))\n",
    "\n",
    "\n",
    "def extract_code_blocks(text: str):\n",
    "    return re.findall(r\"```[\\s\\S]*?```\", text)\n",
    "\n",
    "\n",
    "def extract_sections(text: str):\n",
    "    \"\"\"Split markdown by headings (#, ##, ###...)\"\"\"\n",
    "    sections = re.split(r\"(?=^#{1,6}\\s)\", text, flags=re.MULTILINE)\n",
    "    return [s.strip() for s in sections if s.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f411b265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Markdown files: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================\n",
    "# 4. Load Markdown Files\n",
    "# ==============================\n",
    "markdown_files = list(MARKDOWN_DIR.glob(\"*.md\"))\n",
    "\n",
    "print(f\"Number of Markdown files: {len(markdown_files)}\")\n",
    "\n",
    "raw_documents = []\n",
    "for md_file in markdown_files:\n",
    "    loader = TextLoader(str(md_file), encoding=\"utf-8\")\n",
    "    docs = loader.load()\n",
    "    raw_documents.extend(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8687f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# 5. Exploratory Analysis\n",
    "# ==============================\n",
    "file_stats = []\n",
    "section_analysis = []\n",
    "\n",
    "for doc in raw_documents:\n",
    "    raw_text = doc.page_content\n",
    "    cleaned_text = light_clean_markdown(raw_text)\n",
    "\n",
    "    word_count = count_words(cleaned_text)\n",
    "    code_blocks = extract_code_blocks(cleaned_text)\n",
    "    sections = extract_sections(cleaned_text)\n",
    "\n",
    "    file_stats.append({\n",
    "        \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
    "        \"word_count\": word_count,\n",
    "        \"num_code_blocks\": len(code_blocks),\n",
    "        \"num_sections\": len(sections),\n",
    "    })\n",
    "\n",
    "    for sec in sections:\n",
    "        sec_words = count_words(sec)\n",
    "        sec_code_blocks = extract_code_blocks(sec)\n",
    "\n",
    "        section_analysis.append({\n",
    "            \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
    "            \"section_preview\": sec[:80].replace(\"\\n\", \" \") + \"...\",\n",
    "            \"word_count\": sec_words,\n",
    "            \"code_blocks\": len(sec_code_blocks),\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b4d6609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word count: 16364\n",
      "Average words per file: 3272.80\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================\n",
    "# 6. Aggregate Metrics\n",
    "# ==============================\n",
    "total_words = sum(f[\"word_count\"] for f in file_stats)\n",
    "avg_words = total_words / len(file_stats) if file_stats else 0\n",
    "\n",
    "print(f\"Total word count: {total_words}\")\n",
    "print(f\"Average words per file: {avg_words:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e388633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections with long explanations (>300 words): 6\n",
      "Sections dominated by code blocks: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ==============================\n",
    "# 7. Identify Notable Sections\n",
    "# ==============================\n",
    "LONG_SECTION_THRESHOLD = 300\n",
    "CODE_HEAVY_RATIO = 0.5\n",
    "\n",
    "long_sections = [\n",
    "    s for s in section_analysis\n",
    "    if s[\"word_count\"] >= LONG_SECTION_THRESHOLD\n",
    "]\n",
    "\n",
    "code_dominated_sections = [\n",
    "    s for s in section_analysis\n",
    "    if s[\"code_blocks\"] > 0 and s[\"code_blocks\"] >= CODE_HEAVY_RATIO * max(1, s[\"word_count\"])\n",
    "]\n",
    "\n",
    "print(f\"Sections with long explanations (>{LONG_SECTION_THRESHOLD} words): {len(long_sections)}\"),print(f\"Sections dominated by code blocks: {len(code_dominated_sections)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "433017b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample file stats:\n",
      "{'source': 'data\\\\markdown_docs\\\\boot2.md', 'word_count': 469, 'num_code_blocks': 5, 'num_sections': 20}\n",
      "{'source': 'data\\\\markdown_docs\\\\boot3.md', 'word_count': 325, 'num_code_blocks': 6, 'num_sections': 21}\n",
      "{'source': 'data\\\\markdown_docs\\\\module3.md', 'word_count': 3007, 'num_code_blocks': 0, 'num_sections': 142}\n",
      "\n",
      "Sample long sections:\n",
      "{'source': 'data\\\\markdown_docs\\\\note.md', 'section_preview': '### USER EXPERIENCE DESIGN SOLUTIONS  ``` Please don’t copy the content without ...', 'word_count': 1574, 'code_blocks': 86}\n",
      "{'source': 'data\\\\markdown_docs\\\\note.md', 'section_preview': '### USER EXPERIENCE DESIGN SOLUTIONS  UNIT 2 : PLANNING WEBSITE DESIGN  Please d...', 'word_count': 1516, 'code_blocks': 76}\n",
      "{'source': 'data\\\\markdown_docs\\\\note.md', 'section_preview': '### USER EXPERIENCE DESIGN SOLUTIONS  ``` Please don’t copy the content without ...', 'word_count': 804, 'code_blocks': 44}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================\n",
    "# 8. Sample Outputs (Inspection)\n",
    "# ==============================\n",
    "print(\"\\nSample file stats:\")\n",
    "for f in file_stats[:3]:\n",
    "    print(f)\n",
    "\n",
    "print(\"\\nSample long sections:\")\n",
    "for s in long_sections[:3]:\n",
    "    print(s)\n",
    "\n",
    "# ==============================\n",
    "# 9. Next Step (RAG-ready Output)\n",
    "# ==============================\n",
    "# At this point, cleaned_text per document can be:\n",
    "# - Chunked\n",
    "# - Embedded\n",
    "# - Stored in a vector database\n",
    "# without losing markdown structure or code blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c5f6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/build_vectorstore.py\n",
    "\"\"\"\n",
    "Task 2: Text Chunking, Embedding, and Vector Store Creation\n",
    "\n",
    "Objective:\n",
    "Convert Markdown documents into semantically meaningful chunks,\n",
    "embed them, and store them for fast retrieval.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Configuration\n",
    "# ==============================\n",
    "MARKDOWN_DIR = Path(\"data/markdown_docs\")\n",
    "VECTORSTORE_DIR = Path(\"vectorstore\")\n",
    "VECTORSTORE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 80\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bad14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==============================\n",
    "# Helper Functions\n",
    "# ==============================\n",
    "\n",
    "\n",
    "def contains_code_block(text: str) -> bool:\n",
    "    \"\"\"Check whether a chunk contains a fenced code block.\"\"\"\n",
    "    return bool(re.search(r\"```[\\s\\S]*?```\", text))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_sections_with_headings(text: str):\n",
    "    \"\"\"\n",
    "    Split markdown into (section_heading, section_text) pairs.\n",
    "    If no heading exists, section_heading is None.\n",
    "    \"\"\"\n",
    "    sections = re.split(r\"(?=^#{1,6}\\s)\", text, flags=re.MULTILINE)\n",
    "    results = []\n",
    "\n",
    "\n",
    "    for sec in sections:\n",
    "        sec = sec.strip()\n",
    "        if not sec:\n",
    "            continue\n",
    "\n",
    "\n",
    "        lines = sec.splitlines()\n",
    "        if lines[0].startswith(\"#\"):\n",
    "            heading = lines[0].lstrip(\"#\").strip()\n",
    "            body = \"\\n\".join(lines[1:]).strip()\n",
    "        else:\n",
    "            heading = None\n",
    "            body = sec\n",
    "\n",
    "\n",
    "        results.append((heading, body))\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b262d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 214 section-level documents\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Load Markdown Documents\n",
    "# ==============================\n",
    "from typing import List\n",
    "\n",
    "\n",
    "documents: List[Document] = []\n",
    "\n",
    "for md_file in MARKDOWN_DIR.glob(\"*.md\"):\n",
    "    loader = TextLoader(str(md_file), encoding=\"utf-8\")\n",
    "    loaded_docs = loader.load()\n",
    "\n",
    "    for doc in loaded_docs:\n",
    "        sections = extract_sections_with_headings(doc.page_content)\n",
    "\n",
    "        for heading, section_text in sections:\n",
    "            if not section_text.strip():\n",
    "                continue\n",
    "\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    page_content=section_text,\n",
    "                    metadata={\n",
    "                        \"source_file\": md_file.name,\n",
    "                        \"section_heading\": heading,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "\n",
    "print(f\"Loaded {len(documents)} section-level documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad473e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 520 chunks\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Chunking Strategy\n",
    "# ==============================\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "chunked_documents: List[Document] = []\n",
    "for doc in documents:\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "    for chunk in chunks:\n",
    "        chunked_documents.append(\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    **doc.metadata,\n",
    "                    \"contains_code\": contains_code_block(chunk),\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"Created {len(chunked_documents)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "585d7985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10492\\2039951072.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store successfully saved to 'vectorstore/'\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Embeddings\n",
    "# ==============================\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Vector Store (FAISS)\n",
    "# ==============================\n",
    "vectorstore = FAISS.from_documents(\n",
    "documents=chunked_documents,\n",
    "embedding=embeddings,\n",
    ")\n",
    "\n",
    "\n",
    "vectorstore.save_local(str(VECTORSTORE_DIR))\n",
    "\n",
    "\n",
    "print(\"Vector store successfully saved to 'vectorstore/'\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Notes\n",
    "# ==============================\n",
    "# The saved vector store contains:\n",
    "# - Embedded chunks\n",
    "# - Metadata (source_file, section_heading, contains_code)\n",
    "# - Original chunk text\n",
    "# Ready for fast semantic retrieval in a RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c88f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/rag_pipeline.py\n",
    "\"\"\"\n",
    "Task 3: RAG Core Logic (Retrieval + Generation)\n",
    "Objective:\n",
    "Retrieve relevant Markdown chunks and generate answers strictly\n",
    "from the provided context.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Configuration\n",
    "# ==============================\n",
    "VECTORSTORE_DIR = Path(\"vectorstore\")\n",
    "TOP_K = 5\n",
    "\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL_NAME = \"google/flan-t5-small\" # Streamlit-friendly\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Prompt Template (STRICT RAG)\n",
    "# ==============================\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an educational assistant for web development students.\n",
    "\n",
    "Your task is to answer questions about Bootstrap 5.\n",
    "\n",
    "Answering rules (VERY IMPORTANT):\n",
    "1. First, search for the answer using reliable general knowledge about Bootstrap 5.\n",
    "2. Then, check the provided Context.\n",
    "3. If the Context contains relevant information, prioritize and align your answer with it.\n",
    "4. Do NOT contradict the Context.\n",
    "5. Do NOT guess or invent information.\n",
    "6. If neither outside knowledge nor the Context provides a clear answer, respond exactly with:\n",
    "   \"I cannot find this information in the provided materials.\"\n",
    "\n",
    "Formatting & style rules:\n",
    "- Explain concepts in simple, student-friendly language.\n",
    "- Use Markdown formatting.\n",
    "- When code is relevant, include it inside proper code blocks.\n",
    "- When explaining HTML or Bootstrap classes, clearly describe what each part does.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "input_variables=[\"context\", \"question\"],\n",
    "template=PROMPT_TEMPLATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0637b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Load Vector Store\n",
    "# ==============================\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "str(VECTORSTORE_DIR),\n",
    "embeddings,\n",
    "allow_dangerous_deserialization=True,\n",
    ")\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03ad525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10492\\354045907.py:11: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=text2text_pipeline)\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Load Lightweight LLM\n",
    "# ==============================\n",
    "text2text_pipeline = pipeline(\n",
    "\"text2text-generation\",\n",
    "model=LLM_MODEL_NAME,\n",
    "max_length=512,\n",
    ")\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text2text_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef8fcafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# RAG Answer Function\n",
    "# ==============================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def answer_question(question: str) -> Tuple[str, List[Document]]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant chunks and generate an answer strictly from context.\n",
    "\n",
    "    Returns:\n",
    "    - answer text (str)\n",
    "    - retrieved source documents (List[Document])\n",
    "    \"\"\"\n",
    "\n",
    "    docs = retriever.invoke(question)\n",
    "\n",
    "\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"Source: {d.metadata.get('source_file')} | \"\n",
    "        f\"Section: {d.metadata.get('section_heading')}\\n\"\n",
    "        f\"{d.page_content}\"\n",
    "        for d in docs\n",
    "    )\n",
    "\n",
    "    final_prompt = prompt.format(\n",
    "        context=context,\n",
    "        question=question\n",
    "    )\n",
    "\n",
    "    answer = llm.invoke(final_prompt)\n",
    "\n",
    "\n",
    "    return answer, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "882f6c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " A responsive navigation header used for **site branding and links**.\n",
      "\n",
      "Retrieved Sources:\n",
      "- {'source_file': 'boot2.md', 'section_heading': 'What is a Navbar?', 'contains_code': False}\n",
      "- {'source_file': 'note.md', 'section_heading': 'EXAM PREP', 'contains_code': False}\n",
      "- {'source_file': 'note.md', 'section_heading': 'EXAM PREP', 'contains_code': False}\n",
      "- {'source_file': 'note.md', 'section_heading': 'EXAM PREP', 'contains_code': True}\n",
      "- {'source_file': 'note.md', 'section_heading': 'EXAM PREP', 'contains_code': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==============================\n",
    "# Example Usage (Manual Test)\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    sample_question = \"What does navbar mean in card?\"\n",
    "    response, sources = answer_question(sample_question)\n",
    "\n",
    "\n",
    "    print(\"Answer:\\n\", response)\n",
    "    print(\"\\nRetrieved Sources:\")\n",
    "    for s in sources:\n",
    "        print(\"-\", s.metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
